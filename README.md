# AlphaRAG ğŸ“Š

**Production RAG for Financial Intelligence**

A high-performance Retrieval-Augmented Generation (RAG) system designed for financial document analysis. Built with production-grade optimizations including semantic caching, hybrid search, and sub-200ms P95 latency.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)

---

## ğŸ¯ Key Features

- **High-Performance Retrieval**: Sub-200ms P95 latency with semantic caching and query optimization
- **Hybrid Search**: Combines dense vector search with sparse BM25 for superior recall
- **Smart Chunking**: Semantic document segmentation preserving financial context
- **Citation Tracking**: Exact source attribution with page/paragraph references
- **Cost Optimization**: 60% cost reduction through intelligent caching and prompt compression
- **Production-Ready**: Comprehensive error handling, monitoring, and Docker deployment

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚
â”‚  (Streamlit)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FastAPI Backend                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   Cache    â”‚  â”‚   Rate       â”‚          â”‚
â”‚  â”‚   Layer    â”‚  â”‚   Limiter    â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         RAG Pipeline                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Query   â”‚â†’ â”‚Retrieval â”‚â†’ â”‚Generationâ”‚ â”‚
â”‚  â”‚Optimizer â”‚  â”‚  Engine  â”‚  â”‚  (LLM)   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Data Layer                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Vector  â”‚  â”‚  Redis   â”‚  â”‚ Document â”‚ â”‚
â”‚  â”‚    DB    â”‚  â”‚  Cache   â”‚  â”‚  Store   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

See [docs/architecture.md](docs/architecture.md) for detailed system design.

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- Docker & Docker Compose
- OpenAI API key

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/alpharag.git
cd alpharag
```

2. **Set up environment**
```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

3. **Configure environment variables**
```bash
cp .env.example .env
# Edit .env with your API keys
```

4. **Start services with Docker**
```bash
docker-compose up -d
```

5. **Run the application**
```bash
# Start backend
python src/main.py

# In another terminal, start frontend
streamlit run frontend/app.py
```

Visit `http://localhost:8501` to access the application.

---

## ğŸ“Š Performance Benchmarks

| Metric | Without Optimization | With Optimization | Improvement |
|--------|---------------------|-------------------|-------------|
| **P50 Latency** | 850ms | 165ms | 5.1x faster |
| **P95 Latency** | 1,200ms | 195ms | 6.2x faster |
| **P99 Latency** | 1,800ms | 280ms | 6.4x faster |
| **Cache Hit Rate** | N/A | 42% | N/A |
| **Cost per 1K queries** | $12.50 | $4.80 | 62% reduction |
| **Throughput** | 45 QPS | 180 QPS | 4x increase |

See [benchmarks/](benchmarks/) for detailed performance analysis.

---

## ğŸ“ Project Structure

```
alpharag/
â”œâ”€â”€ src/                      # Source code
â”‚   â”œâ”€â”€ ingestion/           # Document processing pipeline
â”‚   â”œâ”€â”€ retrieval/           # RAG & search components
â”‚   â”œâ”€â”€ optimization/        # Caching & performance
â”‚   â”œâ”€â”€ evaluation/          # Metrics & testing
â”‚   â””â”€â”€ utils/               # Shared utilities
â”œâ”€â”€ frontend/                # Streamlit UI
â”œâ”€â”€ data/                    # Sample documents & datasets
â”œâ”€â”€ tests/                   # Unit & integration tests
â”œâ”€â”€ benchmarks/              # Performance benchmarks
â”œâ”€â”€ docs/                    # Documentation
â”œâ”€â”€ scripts/                 # Utility scripts
â”œâ”€â”€ docker/                  # Docker configurations
â””â”€â”€ config/                  # Configuration files
```

See individual README files in each directory for detailed documentation.

---

## ğŸ”§ Configuration

Key configuration options in `config/config.yaml`:

```yaml
# Model settings
model:
  embedding_model: "all-MiniLM-L6-v2"
  llm_model: "gpt-4-turbo-preview"
  temperature: 0.1

# Retrieval settings
retrieval:
  top_k: 5
  chunk_size: 512
  chunk_overlap: 50
  hybrid_alpha: 0.7  # Weight for dense vs sparse

# Performance
cache:
  enabled: true
  similarity_threshold: 0.95
  ttl: 3600
  
optimization:
  batch_size: 32
  use_reranking: true
```

---

## ğŸ“š Usage Examples

### Basic Query
```python
from src.retrieval.rag_pipeline import RAGPipeline

pipeline = RAGPipeline()
result = pipeline.query(
    "What are the main risk factors in Tesla's 2024 10-K?"
)

print(result.answer)
print(f"Sources: {result.sources}")
print(f"Confidence: {result.confidence}")
```

### Batch Processing
```python
queries = [
    "Compare AAPL and MSFT revenue growth in Q4 2024",
    "What did the Fed announce about interest rates?",
    "Summarize NVDA's AI strategy"
]

results = pipeline.batch_query(queries, batch_size=16)
```

### Custom Retrieval
```python
from src.retrieval.hybrid_search import HybridSearch

searcher = HybridSearch()
docs = searcher.search(
    query="AI investment trends",
    top_k=10,
    filters={"company": "NVDA", "year": 2024}
)
```

See [docs/usage.md](docs/usage.md) for comprehensive examples.

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run with coverage
pytest tests/ --cov=src --cov-report=html

# Run specific test suite
pytest tests/test_retrieval.py -v

# Run benchmarks
python benchmarks/run_benchmarks.py
```

---

## ğŸ“ˆ Evaluation

The system includes comprehensive evaluation metrics:

- **Retrieval Quality**: Precision@K, Recall@K, MRR, NDCG
- **Answer Quality**: Faithfulness, answer relevance, context relevance
- **Performance**: Latency (P50/P95/P99), throughput, cache hit rate
- **Cost**: Token usage, API costs per query

Run evaluation:
```bash
python src/evaluation/run_eval.py --eval-set data/eval/financial_qa.json
```

---

## ğŸ³ Docker Deployment

```bash
# Build images
docker-compose build

# Start all services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

Services:
- **Backend**: `localhost:8000`
- **Frontend**: `localhost:8501`
- **Redis**: `localhost:6379`
- **Vector DB**: `localhost:6333`

---

## ğŸ›£ï¸ Roadmap

- [x] Core RAG pipeline
- [x] Hybrid search implementation
- [x] Semantic caching
- [x] Performance benchmarks
- [ ] Multi-modal support (charts, tables)
- [ ] Fine-tuned reranker model
- [ ] Streaming responses
- [ ] Multi-tenant support
- [ ] Advanced analytics dashboard

---

## ğŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- Built with [LangChain](https://github.com/langchain-ai/langchain) and [LlamaIndex](https://github.com/run-llama/llama_index)
- Powered by [OpenAI](https://openai.com) and [Sentence Transformers](https://www.sbert.net/)
- Inspired by production ML systems at leading financial institutions

---

## ğŸ“§ Contact

**Brian Won** - [injongwbrian@gmail.com](mailto:injongwbrian@gmail.com)

Project Link: [https://github.com/yourusername/alpharag](https://github.com/yourusername/alpharag)

Portfolio: [brianwon.dev](https://brianwon.dev)

---

## ğŸ“Š Project Stats

- **Lines of Code**: ~5,000
- **Test Coverage**: 85%
- **Documentation**: 100% of public APIs
- **Performance Tests**: 50+ scenarios
- **Sample Documents**: 50+ financial filings
